{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.3 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: numpy==2.2.1 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn==1.6.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: optuna==4.1.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (4.1.0)\n",
      "Requirement already satisfied: xgboost==2.1.3 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (2.1.3)\n",
      "Requirement already satisfied: ipykernel==6.29.5 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 13)) (6.29.5)\n",
      "Requirement already satisfied: ipython==8.31.0 in ./venv/lib/python3.12/site-packages (from -r requirements.txt (line 14)) (8.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas==2.2.3->-r requirements.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.12/site-packages (from scikit-learn==1.6.0->-r requirements.txt (line 4)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.12/site-packages (from scikit-learn==1.6.0->-r requirements.txt (line 4)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.12/site-packages (from scikit-learn==1.6.0->-r requirements.txt (line 4)) (3.5.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./venv/lib/python3.12/site-packages (from optuna==4.1.0->-r requirements.txt (line 7)) (1.14.0)\n",
      "Requirement already satisfied: colorlog in ./venv/lib/python3.12/site-packages (from optuna==4.1.0->-r requirements.txt (line 7)) (6.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from optuna==4.1.0->-r requirements.txt (line 7)) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in ./venv/lib/python3.12/site-packages (from optuna==4.1.0->-r requirements.txt (line 7)) (2.0.36)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from optuna==4.1.0->-r requirements.txt (line 7)) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in ./venv/lib/python3.12/site-packages (from optuna==4.1.0->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: appnope in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (1.8.11)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (1.6.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (6.1.1)\n",
      "Requirement already satisfied: pyzmq>=24 in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./venv/lib/python3.12/site-packages (from ipykernel==6.29.5->-r requirements.txt (line 13)) (5.14.3)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.12/site-packages (from ipython==8.31.0->-r requirements.txt (line 14)) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.12/site-packages (from ipython==8.31.0->-r requirements.txt (line 14)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.12/site-packages (from ipython==8.31.0->-r requirements.txt (line 14)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.12/site-packages (from ipython==8.31.0->-r requirements.txt (line 14)) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.12/site-packages (from ipython==8.31.0->-r requirements.txt (line 14)) (2.18.0)\n",
      "Requirement already satisfied: stack_data in ./venv/lib/python3.12/site-packages (from ipython==8.31.0->-r requirements.txt (line 14)) (0.6.3)\n",
      "Requirement already satisfied: Mako in ./venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna==4.1.0->-r requirements.txt (line 7)) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in ./venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna==4.1.0->-r requirements.txt (line 7)) (4.12.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.12/site-packages (from jedi>=0.16->ipython==8.31.0->-r requirements.txt (line 14)) (0.8.4)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel==6.29.5->-r requirements.txt (line 13)) (4.3.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.12/site-packages (from pexpect>4.3->ipython==8.31.0->-r requirements.txt (line 14)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython==8.31.0->-r requirements.txt (line 14)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas==2.2.3->-r requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./venv/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna==4.1.0->-r requirements.txt (line 7)) (3.1.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.12/site-packages (from stack_data->ipython==8.31.0->-r requirements.txt (line 14)) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.12/site-packages (from stack_data->ipython==8.31.0->-r requirements.txt (line 14)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.12/site-packages (from stack_data->ipython==8.31.0->-r requirements.txt (line 14)) (0.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in ./venv/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna==4.1.0->-r requirements.txt (line 7)) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matthieu/Downloads/candidat_2/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id AP     creation_date_answer  situation  ctc  location  gc_id  \\\n",
      "0  cb7a4e0dd0777  f  2019-03-13 07:00:52.562         30  NaN       100     40   \n",
      "1  e78e3915f3e30  f  2019-01-07 13:45:55.741         -1    f        95     40   \n",
      "2  8e65ba155f983  f  2019-01-25 14:01:07.041         -1    f        34     20   \n",
      "3  701e90ca03ce2  f  2019-01-16 14:35:11.432         10    f        45     40   \n",
      "4  768fefec8609a  f  2019-02-11 14:25:37.331         10    f        95    100   \n",
      "\n",
      "  gc_label     creation_date_global       id_group  ... fruit_situation_label  \\\n",
      "0        B  2019-03-13 07:03:13.632  b6a3d931cbbaf  ...                   jzy   \n",
      "1        B  2018-12-18 18:28:41.942  1b35749232404  ...                  hetz   \n",
      "2        D  2018-01-17 13:12:05.124  8f7612ff2c9cc  ...                    ag   \n",
      "3        B  2018-11-07 13:21:33.877  2e3620e03b5f3  ...                    ag   \n",
      "4        H  2018-10-16 10:17:01.716  ac19c1e8abd0d  ...                  hetz   \n",
      "\n",
      "  fruits_or_vegetables  number_of_fruit     id_group_3  \\\n",
      "0                    t                1  bc3a12cac647f   \n",
      "1                    t                1  79aa2c96bd0fc   \n",
      "2                  NaN               -1  4b634a698cc8e   \n",
      "3                  NaN                2  cccd30d947857   \n",
      "4                    f                2  62769fb7addda   \n",
      "\n",
      "     creation_date_request     hobby     id_group_4           ville  \\\n",
      "0  2019-03-13 07:00:52.562  football  b78bd3c9f945c       Saint-Leu   \n",
      "1  2019-01-07 13:45:55.741  football  6fed1653be26d         Créteil   \n",
      "2  2019-01-25 14:01:07.041  football  fb7b5da2ef839        Bordeaux   \n",
      "3  2019-01-16 14:35:11.432  football  3a230e52fb02e  Saint-Herblain   \n",
      "4  2019-02-11 14:25:37.331  football  94c376f28ea60          Drancy   \n",
      "\n",
      "  green_vegetables vegetable_type  \n",
      "0                f            NaN  \n",
      "1                f            NaN  \n",
      "2                f            NaN  \n",
      "3                f            NaN  \n",
      "4                f            NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "base_dir = Path().resolve()  \n",
    "data_dir = base_dir \n",
    "\n",
    "file_path_train = data_dir / \"train.csv\"\n",
    "file_path_test = data_dir / \"test.csv\"\n",
    "\n",
    "train_df = pd.read_csv(file_path_train)\n",
    "test_df = pd.read_csv(file_path_test)\n",
    "\n",
    "\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "1.  **Categorical Variables**:\n",
    "\n",
    "    *   I applied **One Hot Encoding** for categorical columns with a small number of modalities.\n",
    "\n",
    "    *   categorical\\_columns = \\[ 'AP', 'ctc', 'gc\\_label', 'favorite\\_fruit', 'fruit\\_situation\\_label', 'fruits\\_or\\_vegetables', 'hobby', 'green\\_vegetables', 'vegetable\\_type'\\]\n",
    "        \n",
    "    \n",
    "            \n",
    "2.  **Quantitative Variables**:\n",
    "    \n",
    "    *   I identified quantitative columns with a small number of possible values, which might contain either large or small numeric values, both positive and negative.\n",
    "\n",
    "    *   For categorical columns with a large number of unique values (such as 'ville'), I applied **Frequency Encoding**. This involves encoding the values based on the frequency of their occurrences, i.e., the ratio of occurrences of a specific category divided by the total occurrences of all categories, and normalized the values between 0 and 1 to ensure consistent scaling across all features.\n",
    "        \n",
    "    *   columns\\_to\\_check = \\['situation', 'location', 'gc\\_id', 'fruit\\_situation\\_id', 'number\\_of\\_fruit'\\] + \\['ville'\\]\n",
    "        \n",
    "\n",
    "        \n",
    "3.  **Date Columns**:\n",
    "    \n",
    "    *   After extracting these components, I applied **Frequency Encoding** to each of the newly created columns (i.e., year, month, and day) to encode the temporal information.\n",
    "\n",
    "     *   date\\_columns = \\['creation\\_date\\_answer', 'creation\\_date\\_global', 'creation\\_date\\_request'\\]\n",
    "        \n",
    "\n",
    "\n",
    "4.  **Id Columns**:\n",
    "    \n",
    "    *   id\\_columns = \\['id\\_group', 'id\\_group\\_2', 'id\\_group\\_3', 'id\\_group\\_4'\\]\n",
    "        \n",
    "    *   These columns had a large number of unique values, with approximately 15,000 unique IDs for some, making them likely not very informative for model training. Therefore, I decided to **drop** these columns, assuming they did not provide significant value.\n",
    "        \n",
    "5.  **Columns to Remove**:\n",
    "    \n",
    "    *   columns\\_to\\_remove = \\[ 'creation\\_date\\_answer', 'creation\\_date\\_global', 'id\\_group', 'id\\_group\\_2', 'id\\_group\\_3', 'creation\\_date\\_request', 'id\\_group\\_4'\\]\n",
    "        \n",
    "6.  **Final Design Matrix**:\n",
    "    \n",
    "    *   I replace in the end all missing values by 0. After encoding all the variables into real numbers between 0 and 1, I had the final **design matrix** ready for model training, with the **'id'** column retained to preserve the identity of each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id  target  situation_encoded  location_encoded  gc_id_encoded  \\\n",
      "0  a46cfa61ea20a       0             0.9586           0.00908        0.02100   \n",
      "1  c3d0cb8f0c5e2       1             0.9586           0.01876        0.55252   \n",
      "2  05dfbe0ec3a8b       0             0.9586           0.04980        0.55252   \n",
      "3  952e869ee1076       1             0.9586           0.01848        0.55252   \n",
      "4  5bd0e71b1395b       1             0.9586           0.01016        0.15416   \n",
      "\n",
      "   fruit_situation_id_encoded  number_of_fruit_encoded  ville_encoded  \\\n",
      "0                     0.03732                  0.71672            0.0   \n",
      "1                     0.28764                  0.71672            0.0   \n",
      "2                     0.42604                  0.71672            0.0   \n",
      "3                     0.28764                  0.21260            0.0   \n",
      "4                     0.28764                  0.71672            0.0   \n",
      "\n",
      "   creation_date_answer_year_encoded  creation_date_global_year_encoded  ...  \\\n",
      "0                                1.0                            0.40516  ...   \n",
      "1                                1.0                            0.51692  ...   \n",
      "2                                1.0                            0.51692  ...   \n",
      "3                                1.0                            0.51692  ...   \n",
      "4                                1.0                            0.51692  ...   \n",
      "\n",
      "   hobby_noball  hobby_volleyball  green_vegetables_f  green_vegetables_t  \\\n",
      "0             0                 0                   1                   0   \n",
      "1             0                 0                   1                   0   \n",
      "2             0                 0                   1                   0   \n",
      "3             0                 0                   1                   0   \n",
      "4             0                 0                   1                   0   \n",
      "\n",
      "   vegetable_type_almostgreen  vegetable_type_green  \\\n",
      "0                           0                     0   \n",
      "1                           0                     0   \n",
      "2                           0                     0   \n",
      "3                           0                     0   \n",
      "4                           0                     0   \n",
      "\n",
      "   vegetable_type_notsogreen  vegetable_type_prettygreen  \\\n",
      "0                          0                           0   \n",
      "1                          0                           0   \n",
      "2                          0                           0   \n",
      "3                          0                           0   \n",
      "4                          0                           0   \n",
      "\n",
      "   vegetable_type_salad  vegetable_type_verygreen  \n",
      "0                     0                         0  \n",
      "1                     0                         0  \n",
      "2                     0                         0  \n",
      "3                     0                         0  \n",
      "4                     0                         0  \n",
      "\n",
      "[5 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "class DummyEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Dummy Encoder for performing One-Hot Encoding on categorical columns,\n",
    "    Frequency Encoding on 'ville', numeric columns, and date-related columns,\n",
    "    and dropping specific columns. Also, extracts date components for specific date columns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, categorial_columns: list = None,\n",
    "                 columns_to_drop: list = None, \n",
    "                 columns_to_check: list = None,\n",
    "                 date_columns: list = None):\n",
    "        \"\"\"\n",
    "        Initializes the DummyEncoder.\n",
    "\n",
    "        Parameters:\n",
    "        categorial_columns : list, optional\n",
    "            List of categorical column names to encode (except 'ville').\n",
    "        columns_to_drop : list, optional\n",
    "            List of column names to drop.\n",
    "        columns_to_check : list, optional\n",
    "            List of columns to apply frequency encoding to (including 'ville').\n",
    "        date_columns : list, optional\n",
    "            List of date columns to extract year, month, and day components.\n",
    "        \"\"\"\n",
    "        self.categorial_columns = categorial_columns\n",
    "        self.columns_to_drop = columns_to_drop if columns_to_drop is not None else []\n",
    "        self.columns_to_check = columns_to_check if columns_to_check is not None else []\n",
    "        self.date_columns = date_columns if date_columns is not None else []\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y: np.ndarray = None) -> 'DummyEncoder':\n",
    "        \"\"\"\n",
    "        Fits the encoder to the input data, learning the unique categories for each categorical column.\n",
    "\n",
    "        Parameters:\n",
    "        X : pandas DataFrame\n",
    "            The input data to fit the encoder on.\n",
    "        y : None\n",
    "            Not used, but required for compatibility with scikit-learn API.\n",
    "\n",
    "        Returns:\n",
    "        self : returns an instance of self.\n",
    "        \"\"\"\n",
    "        self.unique_categories_ = {}\n",
    "        if self.categorial_columns is None:\n",
    "            self.categorial_columns = X.select_dtypes(include=['object', 'category']).columns\n",
    "        for col in self.categorial_columns:\n",
    "            self.unique_categories_[col] = X[col].dropna().unique()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transforms the input data by applying One-Hot Encoding to the categorical columns,\n",
    "        Frequency Encoding to 'ville' and numeric columns, and dropping specific columns.\n",
    "\n",
    "        Parameters:\n",
    "        X : pandas DataFrame\n",
    "            The input data to transform.\n",
    "\n",
    "        Returns:\n",
    "        X_transformed : pandas DataFrame\n",
    "            The transformed data with encoded columns and specific columns dropped.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed = self.extract_date_components(X_transformed)\n",
    "        \n",
    "        # Drop specified columns\n",
    "        X_transformed = self.drop_columns(X_transformed)\n",
    "        \n",
    "        # Frequency encoding for specific columns including date components\n",
    "        for col in self.columns_to_check + ['ville'] + [col + '_year' for col in self.date_columns] + \\\n",
    "            [col + '_month' for col in self.date_columns] + [col + '_day' for col in self.date_columns]:\n",
    "            if col in X_transformed:\n",
    "                X_transformed = self.encode_by_frequency(X_transformed, col)\n",
    "        \n",
    "        # One-Hot Encoding for categorical columns\n",
    "        for col in self.unique_categories_.keys():\n",
    "            if col in X_transformed:\n",
    "                X_transformed = self.encode_by_onehot(X_transformed, col)\n",
    "        \n",
    "        # Replace NaN with 0 and boolean values with 0/1\n",
    "        X_transformed.fillna(0, inplace=True)\n",
    "        X_transformed = X_transformed.replace({True: 1, False: 0})\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "    def encode_by_frequency(self, X: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encodes a categorical or numeric column by the frequency of each category or value.\n",
    "\n",
    "        Parameters:\n",
    "        X : pandas DataFrame\n",
    "            The input data.\n",
    "        column : str\n",
    "            The column to be encoded.\n",
    "\n",
    "        Returns:\n",
    "        X : pandas DataFrame\n",
    "            The data with the frequency-encoded column.\n",
    "        \"\"\"\n",
    "        if column in X.columns:\n",
    "            freq_encoding = X[column].value_counts(normalize=True)\n",
    "            X[column + '_encoded'] = X[column].map(freq_encoding)\n",
    "            X = X.drop(columns=[column])\n",
    "        return X\n",
    "\n",
    "    def encode_by_onehot(self, X: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Encodes a categorical column by One-Hot Encoding.\n",
    "\n",
    "        Parameters:\n",
    "        X : pandas DataFrame\n",
    "            The input data.\n",
    "        column : str\n",
    "            The column to be encoded.\n",
    "\n",
    "        Returns:\n",
    "        X : pandas DataFrame\n",
    "            The data with One-Hot encoded column.\n",
    "        \"\"\"\n",
    "        if column in X.columns:\n",
    "            dummies = pd.get_dummies(X[column], prefix=column)\n",
    "            X = X.drop(columns=[column])\n",
    "            X = pd.concat([X, dummies], axis=1)\n",
    "        return X\n",
    "\n",
    "    def drop_columns(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Drops specific columns from the DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        X : pandas DataFrame\n",
    "            The input data containing columns to be dropped.\n",
    "\n",
    "        Returns:\n",
    "        X : pandas DataFrame\n",
    "            The input data with specific columns removed.\n",
    "        \"\"\"\n",
    "        X = X.drop(columns=self.columns_to_drop, errors='ignore')\n",
    "        return X\n",
    "    \n",
    "    def extract_date_components(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extracts year, month, and day from specified date columns and creates new columns.\n",
    "        \n",
    "        Parameters:\n",
    "        df : pandas DataFrame\n",
    "            The DataFrame with date columns.\n",
    "        \n",
    "        Returns:\n",
    "        df : pandas DataFrame\n",
    "            The DataFrame with new year, month, and day columns for each specified date column.\n",
    "        \"\"\"\n",
    "        for col in self.date_columns:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "                \n",
    "                if df[col].isnull().all():\n",
    "                    print(f\"Warning: Column {col} could not be converted to datetime.\")\n",
    "                    continue\n",
    "\n",
    "                df[col + '_year'] = df[col].dt.year\n",
    "                df[col + '_month'] = df[col].dt.month\n",
    "                df[col + '_day'] = df[col].dt.day\n",
    "        return df\n",
    "\n",
    "\n",
    "columns_to_remove = [\n",
    "    'creation_date_answer', 'creation_date_global', 'id_group', \n",
    "    'id_group_2', 'id_group_3', 'creation_date_request', 'id_group_4'\n",
    "]\n",
    "columns_to_check = ['situation', 'location', 'gc_id', 'fruit_situation_id', 'number_of_fruit']\n",
    "date_columns = ['creation_date_answer', 'creation_date_global', 'creation_date_request']\n",
    "\n",
    "# Initialize the DummyEncoder\n",
    "encoder = DummyEncoder(\n",
    "    categorial_columns=[\n",
    "        'AP', 'ctc', 'gc_label', 'favorite_fruit', 'fruit_situation_label', \n",
    "        'fruits_or_vegetables', 'hobby', 'green_vegetables', 'vegetable_type'\n",
    "    ], \n",
    "    columns_to_drop=columns_to_remove,\n",
    "    columns_to_check=columns_to_check,\n",
    "    date_columns=date_columns  \n",
    ")\n",
    "\n",
    "# Prepare and encode the data\n",
    "df_train = train_df.copy()\n",
    "df_train.columns = df_train.columns.str.strip()  # Ensure columns are stripped of extra spaces\n",
    "df_encoded_train = encoder.fit_transform(df_train)\n",
    "\n",
    "# Save the final encoded dataframe with the new date columns\n",
    "file_path_dir = data_dir / \"design_matrix.csv\"\n",
    "df_encoded_train.to_csv(file_path_dir, index=False)\n",
    "\n",
    "print(df_encoded_train.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Splitting**\n",
    "\n",
    "*   **Training Set**: 60% of the original dataset.\n",
    "    \n",
    "*   **Validation Set**: 20%.\n",
    "    \n",
    "*   **Test Set**: 20%.\n",
    "    \n",
    "\n",
    "#### **Models Used**\n",
    "\n",
    "1.  Logistic Regression\n",
    "    \n",
    "2.  Random Forest\n",
    "    \n",
    "3.  Gradient Boosting\n",
    "    \n",
    "4.  K Nearest Neighbors\n",
    "    \n",
    "5.  Extra Trees\n",
    "    \n",
    "6.  AdaBoost\n",
    "    \n",
    "7.  XGBoost (with **Optuna** for hyperparameter tuning due to compatibility issues with scikit-learn).\n",
    "\n",
    "I tuned parameters of each model with cross validation.\n",
    "I kept for each model the optimal set of parameters.\n",
    "\n",
    "    \n",
    "\n",
    "#### **Evaluation Metrics**\n",
    "\n",
    "Each model was evaluated using:\n",
    "\n",
    "*   Log-Loss for train, validation, and test sets.\n",
    "    \n",
    "*   Accuracy for train, validation, and test sets.\n",
    "    \n",
    "\n",
    "#### **Predictions**\n",
    "\n",
    "*   Saved in a directory named **predictions**:\n",
    "    \n",
    "    *   **all-1**: Contains predictions from all models except the best-performing one.\n",
    "        \n",
    "    *   **best\\_predictions**: Contains predictions from the model with the best performance.\n",
    "        \n",
    "\n",
    "#### **Format of Predictions**\n",
    "\n",
    "Each file includes:\n",
    "\n",
    "*   id: Identifier for the sample.\n",
    "    \n",
    "*   class 0, class 1, class 2, class 3: Probabilities of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logreg model...\n",
      "Best parameters for logreg: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "logreg Train Log-Loss: 0.8283\n",
      "logreg Validation Log-Loss: 0.8334\n",
      "logreg Test Log-Loss: 0.8823\n",
      "logreg Train Accuracy: 68.37%\n",
      "logreg Validation Accuracy: 68.36%\n",
      "logreg Test Accuracy: 66.00%\n",
      "Predictions for logreg saved to /Users/matthieu/Downloads/candidat_2/predictions_logreg.csv\n",
      "Training rf model...\n",
      "Best parameters for rf: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "rf Train Log-Loss: 0.4321\n",
      "rf Validation Log-Loss: 0.7920\n",
      "rf Test Log-Loss: 0.8221\n",
      "rf Train Accuracy: 84.42%\n",
      "rf Validation Accuracy: 69.76%\n",
      "rf Test Accuracy: 67.32%\n",
      "Predictions for rf saved to /Users/matthieu/Downloads/candidat_2/predictions_rf.csv\n",
      "Training gb model...\n",
      "Best parameters for gb: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "gb Train Log-Loss: 0.6358\n",
      "gb Validation Log-Loss: 0.7086\n",
      "gb Test Log-Loss: 0.7396\n",
      "gb Train Accuracy: 73.93%\n",
      "gb Validation Accuracy: 71.54%\n",
      "gb Test Accuracy: 69.40%\n",
      "Predictions for gb saved to /Users/matthieu/Downloads/candidat_2/predictions_gb.csv\n",
      "Training knn model...\n",
      "Best parameters for knn: {'n_neighbors': 7, 'p': 1, 'weights': 'uniform'}\n",
      "knn Train Log-Loss: 0.6035\n",
      "knn Validation Log-Loss: 3.5365\n",
      "knn Test Log-Loss: 3.8826\n",
      "knn Train Accuracy: 72.51%\n",
      "knn Validation Accuracy: 66.46%\n",
      "knn Test Accuracy: 64.28%\n",
      "Predictions for knn saved to /Users/matthieu/Downloads/candidat_2/predictions_knn.csv\n",
      "Training et model...\n",
      "Best parameters for et: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "et Train Log-Loss: 0.5769\n",
      "et Validation Log-Loss: 0.7805\n",
      "et Test Log-Loss: 0.8262\n",
      "et Train Accuracy: 74.80%\n",
      "et Validation Accuracy: 69.34%\n",
      "et Test Accuracy: 66.82%\n",
      "Predictions for et saved to /Users/matthieu/Downloads/candidat_2/predictions_et.csv\n",
      "Training ada model...\n",
      "Best parameters for ada: {'learning_rate': 0.1, 'n_estimators': 50}\n",
      "ada Train Log-Loss: 1.2890\n",
      "ada Validation Log-Loss: 1.2884\n",
      "ada Test Log-Loss: 1.2934\n",
      "ada Train Accuracy: 67.23%\n",
      "ada Validation Accuracy: 66.96%\n",
      "ada Test Accuracy: 64.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 18:54:47,672] A new study created in memory with name: no-name-2b0a04fd-6f3f-4d0d-9471-7c27052ebfb4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for ada saved to /Users/matthieu/Downloads/candidat_2/predictions_ada.csv\n",
      "Training XGBoost with Optuna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 18:54:48,660] Trial 0 finished with value: 0.6884694128690173 and parameters: {'max_depth': 5, 'learning_rate': 0.19967460122857034, 'n_estimators': 101, 'subsample': 0.9717421661042149, 'colsample_bytree': 0.8122778692171073}. Best is trial 0 with value: 0.6884694128690173.\n",
      "[I 2024-12-26 18:54:49,871] Trial 1 finished with value: 0.7128756212996743 and parameters: {'max_depth': 5, 'learning_rate': 0.07284851557451076, 'n_estimators': 120, 'subsample': 0.9411676055756003, 'colsample_bytree': 0.8742518081125685}. Best is trial 0 with value: 0.6884694128690173.\n",
      "[I 2024-12-26 18:54:50,814] Trial 2 finished with value: 0.7035637076901017 and parameters: {'max_depth': 7, 'learning_rate': 0.09840995495586018, 'n_estimators': 58, 'subsample': 0.8569691137962188, 'colsample_bytree': 0.8840337391313009}. Best is trial 0 with value: 0.6884694128690173.\n",
      "[I 2024-12-26 18:54:53,296] Trial 3 finished with value: 0.7226665195517653 and parameters: {'max_depth': 10, 'learning_rate': 0.15656480520755245, 'n_estimators': 119, 'subsample': 0.8309318893978818, 'colsample_bytree': 0.8818450276329992}. Best is trial 0 with value: 0.6884694128690173.\n",
      "[I 2024-12-26 18:54:54,363] Trial 4 finished with value: 0.7126275929421446 and parameters: {'max_depth': 8, 'learning_rate': 0.0778332150746108, 'n_estimators': 56, 'subsample': 0.8984043159344767, 'colsample_bytree': 0.9738943690089752}. Best is trial 0 with value: 0.6884694128690173.\n",
      "[I 2024-12-26 18:54:55,427] Trial 5 finished with value: 0.689978232730899 and parameters: {'max_depth': 9, 'learning_rate': 0.16584135879876177, 'n_estimators': 74, 'subsample': 0.9507492531952181, 'colsample_bytree': 0.9206971062942455}. Best is trial 0 with value: 0.6884694128690173.\n",
      "[I 2024-12-26 18:54:58,196] Trial 6 finished with value: 0.7249405662649065 and parameters: {'max_depth': 9, 'learning_rate': 0.1400572849446265, 'n_estimators': 188, 'subsample': 0.8758331124299049, 'colsample_bytree': 0.8478036783756809}. Best is trial 0 with value: 0.6884694128690173.\n",
      "[I 2024-12-26 18:54:59,224] Trial 7 finished with value: 0.6867010139776805 and parameters: {'max_depth': 7, 'learning_rate': 0.1404665166325772, 'n_estimators': 95, 'subsample': 0.920956386679997, 'colsample_bytree': 0.8412083304328133}. Best is trial 7 with value: 0.6867010139776805.\n",
      "[I 2024-12-26 18:55:01,627] Trial 8 finished with value: 0.6835136958145117 and parameters: {'max_depth': 9, 'learning_rate': 0.054283968431583327, 'n_estimators': 172, 'subsample': 0.8952842867014866, 'colsample_bytree': 0.8801526209364877}. Best is trial 8 with value: 0.6835136958145117.\n",
      "[I 2024-12-26 18:55:02,123] Trial 9 finished with value: 0.7474630769606846 and parameters: {'max_depth': 3, 'learning_rate': 0.11194796840591276, 'n_estimators': 90, 'subsample': 0.9391984441020601, 'colsample_bytree': 0.8499343350310182}. Best is trial 8 with value: 0.6835136958145117.\n",
      "[I 2024-12-26 18:55:05,491] Trial 10 finished with value: 0.746188608735774 and parameters: {'max_depth': 10, 'learning_rate': 0.014987830478556285, 'n_estimators': 175, 'subsample': 0.8027204054042731, 'colsample_bytree': 0.9345424444318974}. Best is trial 8 with value: 0.6835136958145117.\n",
      "[I 2024-12-26 18:55:07,972] Trial 11 finished with value: 0.7435644456580734 and parameters: {'max_depth': 7, 'learning_rate': 0.021809096900310868, 'n_estimators': 154, 'subsample': 0.9125200606103321, 'colsample_bytree': 0.8144334447016277}. Best is trial 8 with value: 0.6835136958145117.\n",
      "[I 2024-12-26 18:55:09,500] Trial 12 finished with value: 0.7085716053319933 and parameters: {'max_depth': 6, 'learning_rate': 0.04858216873571224, 'n_estimators': 150, 'subsample': 0.9975446816827684, 'colsample_bytree': 0.8426470845787557}. Best is trial 8 with value: 0.6835136958145117.\n",
      "[I 2024-12-26 18:55:11,858] Trial 13 finished with value: 0.6894439031700856 and parameters: {'max_depth': 8, 'learning_rate': 0.12350936326451914, 'n_estimators': 146, 'subsample': 0.9036446971883385, 'colsample_bytree': 0.9164486588901637}. Best is trial 8 with value: 0.6835136958145117.\n",
      "[I 2024-12-26 18:55:14,357] Trial 14 finished with value: 0.6827336769060864 and parameters: {'max_depth': 8, 'learning_rate': 0.04876055958626866, 'n_estimators': 198, 'subsample': 0.8704321501084421, 'colsample_bytree': 0.9640534931878149}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:16,966] Trial 15 finished with value: 0.6845512226301533 and parameters: {'max_depth': 9, 'learning_rate': 0.043369234966923007, 'n_estimators': 195, 'subsample': 0.864268765421902, 'colsample_bytree': 0.9979916504005883}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:19,083] Trial 16 finished with value: 0.6840804478894454 and parameters: {'max_depth': 8, 'learning_rate': 0.0559685946113468, 'n_estimators': 169, 'subsample': 0.8319567919299463, 'colsample_bytree': 0.9447011423764252}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:22,554] Trial 17 finished with value: 0.7096575226202448 and parameters: {'max_depth': 10, 'learning_rate': 0.0854905771214161, 'n_estimators': 199, 'subsample': 0.88224369744528, 'colsample_bytree': 0.9618980453015633}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:25,071] Trial 18 finished with value: 0.7194627795052493 and parameters: {'max_depth': 6, 'learning_rate': 0.032346067591754804, 'n_estimators': 167, 'subsample': 0.8439697432622905, 'colsample_bytree': 0.904732993093547}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:25,818] Trial 19 finished with value: 0.753408493427326 and parameters: {'max_depth': 3, 'learning_rate': 0.06338068080275969, 'n_estimators': 136, 'subsample': 0.8886067006661201, 'colsample_bytree': 0.9997430143261221}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:28,505] Trial 20 finished with value: 0.8029097335736849 and parameters: {'max_depth': 9, 'learning_rate': 0.010233739058271046, 'n_estimators': 183, 'subsample': 0.8054366086099637, 'colsample_bytree': 0.9662514014789965}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:30,369] Trial 21 finished with value: 0.6846446978609303 and parameters: {'max_depth': 8, 'learning_rate': 0.054298808018786844, 'n_estimators': 166, 'subsample': 0.82778552611626, 'colsample_bytree': 0.9500674748714548}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:32,360] Trial 22 finished with value: 0.6935249218253678 and parameters: {'max_depth': 8, 'learning_rate': 0.035461848983989505, 'n_estimators': 177, 'subsample': 0.8572528527421285, 'colsample_bytree': 0.9340834825441945}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:34,555] Trial 23 finished with value: 0.6832555626499763 and parameters: {'max_depth': 8, 'learning_rate': 0.061404776072186196, 'n_estimators': 200, 'subsample': 0.8287059196355598, 'colsample_bytree': 0.9457627274041607}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:37,077] Trial 24 finished with value: 0.694788212246412 and parameters: {'max_depth': 9, 'learning_rate': 0.08566391936676093, 'n_estimators': 199, 'subsample': 0.8452404571232011, 'colsample_bytree': 0.8999616406044669}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:38,789] Trial 25 finished with value: 0.6839089006754271 and parameters: {'max_depth': 7, 'learning_rate': 0.06607986191323818, 'n_estimators': 186, 'subsample': 0.8696786287870767, 'colsample_bytree': 0.9789019168686603}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:44,035] Trial 26 finished with value: 0.6883688159183458 and parameters: {'max_depth': 10, 'learning_rate': 0.03125457456129932, 'n_estimators': 188, 'subsample': 0.9238141994265178, 'colsample_bytree': 0.8631440640935923}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:45,334] Trial 27 finished with value: 0.6838840761100917 and parameters: {'max_depth': 6, 'learning_rate': 0.09424181046966108, 'n_estimators': 160, 'subsample': 0.8191461669855118, 'colsample_bytree': 0.9523451225792537}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:46,824] Trial 28 finished with value: 0.6861750036437492 and parameters: {'max_depth': 8, 'learning_rate': 0.11116027472473196, 'n_estimators': 135, 'subsample': 0.8985054928811873, 'colsample_bytree': 0.9849787758219898}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:47,959] Trial 29 finished with value: 0.6873061579843767 and parameters: {'max_depth': 5, 'learning_rate': 0.18226210467986936, 'n_estimators': 178, 'subsample': 0.8431051881274041, 'colsample_bytree': 0.9241702046384493}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:50,448] Trial 30 finished with value: 0.6848414032122287 and parameters: {'max_depth': 9, 'learning_rate': 0.039742387735971677, 'n_estimators': 192, 'subsample': 0.8870210275852675, 'colsample_bytree': 0.9028068014052488}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:51,492] Trial 31 finished with value: 0.6895346848264841 and parameters: {'max_depth': 5, 'learning_rate': 0.1010401439976476, 'n_estimators': 159, 'subsample': 0.8129298948452178, 'colsample_bytree': 0.9539751874286563}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:52,767] Trial 32 finished with value: 0.6871845475981562 and parameters: {'max_depth': 6, 'learning_rate': 0.07602874541204141, 'n_estimators': 162, 'subsample': 0.8178382492540149, 'colsample_bytree': 0.9374986503005904}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:53,424] Trial 33 finished with value: 0.7218297563314724 and parameters: {'max_depth': 4, 'learning_rate': 0.0899651433861008, 'n_estimators': 111, 'subsample': 0.8532833199266345, 'colsample_bytree': 0.9628420149638821}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:54,764] Trial 34 finished with value: 0.6862992171022689 and parameters: {'max_depth': 7, 'learning_rate': 0.0666477522078607, 'n_estimators': 140, 'subsample': 0.8231559420525978, 'colsample_bytree': 0.8903650553745818}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:56,128] Trial 35 finished with value: 0.6928452082128974 and parameters: {'max_depth': 6, 'learning_rate': 0.05933699699319871, 'n_estimators': 175, 'subsample': 0.8369667576333748, 'colsample_bytree': 0.8644618707239616}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:55:57,810] Trial 36 finished with value: 0.683173622541982 and parameters: {'max_depth': 7, 'learning_rate': 0.07724717529542956, 'n_estimators': 182, 'subsample': 0.8634456842520046, 'colsample_bytree': 0.9140745960254506}. Best is trial 14 with value: 0.6827336769060864.\n",
      "[I 2024-12-26 18:56:00,654] Trial 37 finished with value: 0.6823559844193108 and parameters: {'max_depth': 7, 'learning_rate': 0.07164749533319968, 'n_estimators': 200, 'subsample': 0.8698474123488086, 'colsample_bytree': 0.9135200285538384}. Best is trial 37 with value: 0.6823559844193108.\n",
      "[I 2024-12-26 18:56:02,746] Trial 38 finished with value: 0.6830648971653349 and parameters: {'max_depth': 7, 'learning_rate': 0.07461962305770642, 'n_estimators': 199, 'subsample': 0.8703416887332487, 'colsample_bytree': 0.9159145134374538}. Best is trial 37 with value: 0.6823559844193108.\n",
      "[I 2024-12-26 18:56:04,592] Trial 39 finished with value: 0.68217779217132 and parameters: {'max_depth': 7, 'learning_rate': 0.07765286660733475, 'n_estimators': 184, 'subsample': 0.8721640510064187, 'colsample_bytree': 0.9185009730688727}. Best is trial 39 with value: 0.68217779217132.\n",
      "[I 2024-12-26 18:56:06,544] Trial 40 finished with value: 0.6902185104003613 and parameters: {'max_depth': 7, 'learning_rate': 0.12386486532923552, 'n_estimators': 191, 'subsample': 0.8761873823989584, 'colsample_bytree': 0.924030180812199}. Best is trial 39 with value: 0.68217779217132.\n",
      "[I 2024-12-26 18:56:08,447] Trial 41 finished with value: 0.6821418907917643 and parameters: {'max_depth': 7, 'learning_rate': 0.07620846314180782, 'n_estimators': 181, 'subsample': 0.8642805932262143, 'colsample_bytree': 0.9116780897768078}. Best is trial 41 with value: 0.6821418907917643.\n",
      "[I 2024-12-26 18:56:10,793] Trial 42 finished with value: 0.6824656422290276 and parameters: {'max_depth': 7, 'learning_rate': 0.07280432147434698, 'n_estimators': 192, 'subsample': 0.8712877469840988, 'colsample_bytree': 0.8927498503346613}. Best is trial 41 with value: 0.6821418907917643.\n",
      "[I 2024-12-26 18:56:12,677] Trial 43 finished with value: 0.6842086605847519 and parameters: {'max_depth': 6, 'learning_rate': 0.08295698048090623, 'n_estimators': 185, 'subsample': 0.8534926802356883, 'colsample_bytree': 0.8888792673428326}. Best is trial 41 with value: 0.6821418907917643.\n",
      "[I 2024-12-26 18:56:14,565] Trial 44 finished with value: 0.6874001342079937 and parameters: {'max_depth': 7, 'learning_rate': 0.047458537729050475, 'n_estimators': 194, 'subsample': 0.8799078089543532, 'colsample_bytree': 0.8719523044480814}. Best is trial 41 with value: 0.6821418907917643.\n",
      "[I 2024-12-26 18:56:16,591] Trial 45 finished with value: 0.690288466540442 and parameters: {'max_depth': 8, 'learning_rate': 0.10146880307761867, 'n_estimators': 181, 'subsample': 0.9074334404661919, 'colsample_bytree': 0.898535914728397}. Best is trial 41 with value: 0.6821418907917643.\n",
      "[I 2024-12-26 18:56:17,422] Trial 46 finished with value: 0.7062199622430188 and parameters: {'max_depth': 7, 'learning_rate': 0.0703717881272851, 'n_estimators': 77, 'subsample': 0.8911576374289741, 'colsample_bytree': 0.9117959387229078}. Best is trial 41 with value: 0.6821418907917643.\n",
      "[I 2024-12-26 18:56:19,002] Trial 47 finished with value: 0.7238506832443152 and parameters: {'max_depth': 6, 'learning_rate': 0.026136589561683694, 'n_estimators': 189, 'subsample': 0.8624580413364666, 'colsample_bytree': 0.9302368671252256}. Best is trial 41 with value: 0.6821418907917643.\n",
      "[I 2024-12-26 18:56:21,084] Trial 48 finished with value: 0.6857194588317466 and parameters: {'max_depth': 8, 'learning_rate': 0.10940098977675523, 'n_estimators': 121, 'subsample': 0.8726606438123938, 'colsample_bytree': 0.8871585133317751}. Best is trial 41 with value: 0.6821418907917643.\n",
      "[I 2024-12-26 18:56:22,787] Trial 49 finished with value: 0.6893999871359583 and parameters: {'max_depth': 7, 'learning_rate': 0.05088173027866419, 'n_estimators': 171, 'subsample': 0.920089878626445, 'colsample_bytree': 0.8297077816257512}. Best is trial 41 with value: 0.6821418907917643.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb Train Log-Loss: 0.4631\n",
      "xgb Validation Log-Loss: 0.6821\n",
      "xgb Test Log-Loss: 0.7182\n",
      "xgb Train Accuracy: 81.83%\n",
      "xgb Validation Accuracy: 71.86%\n",
      "xgb Test Accuracy: 69.94%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for xgb saved to /Users/matthieu/Downloads/candidat_2/predictions_xgb.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare data function\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Prepares the data by separating the features (X) and target (y).\n",
    "    \n",
    "    Parameters:\n",
    "    df : pandas DataFrame\n",
    "        The dataset containing the target column and features.\n",
    "\n",
    "    Returns:\n",
    "    X : pandas DataFrame\n",
    "        The feature set excluding target and id.\n",
    "    y : pandas Series\n",
    "        The target column.\n",
    "    \"\"\"\n",
    "    X = df.drop(columns=['target', 'id']).copy()  \n",
    "    y = df['target'].copy()\n",
    "    return X, y\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "def split_data(X, y):\n",
    "    \"\"\"\n",
    "    Splits the data into training, validation, and test sets.\n",
    "    \n",
    "    Parameters:\n",
    "    X : pandas DataFrame\n",
    "        The features.\n",
    "    y : pandas Series\n",
    "        The target variable.\n",
    "    \n",
    "    Returns:\n",
    "    X_train, X_val, X_test : pandas DataFrame\n",
    "        The training, validation, and test feature sets.\n",
    "    y_train, y_val, y_test : pandas Series\n",
    "        The training, validation, and test target sets.\n",
    "    \"\"\"\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Optuna hyperparameter optimization for XGBoost\n",
    "def objective(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize XGBoost hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    trial : optuna.Trial\n",
    "        The trial object for hyperparameter search.\n",
    "    X_train : pandas DataFrame\n",
    "        The training features.\n",
    "    y_train : pandas Series\n",
    "        The training target variable.\n",
    "    X_val : pandas DataFrame\n",
    "        The validation features.\n",
    "    y_val : pandas Series\n",
    "        The validation target variable.\n",
    "    \n",
    "    Returns:\n",
    "    float\n",
    "        The log loss value for the validation set.\n",
    "    \"\"\"\n",
    "    model = XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=len(y_train.unique()),\n",
    "        max_depth=trial.suggest_int('max_depth', 3, 10),\n",
    "        learning_rate=trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        n_estimators=trial.suggest_int('n_estimators', 50, 200),\n",
    "        subsample=trial.suggest_float('subsample', 0.8, 1.0),\n",
    "        colsample_bytree=trial.suggest_float('colsample_bytree', 0.8, 1.0),\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_val = model.predict_proba(X_val)\n",
    "    return log_loss(y_val, y_pred_val)\n",
    "\n",
    "# Train XGBoost using Optuna for hyperparameter tuning\n",
    "def tune_xgb_with_optuna(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Tunes hyperparameters of the XGBoost model using Optuna.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train : pandas DataFrame\n",
    "        The training features.\n",
    "    y_train : pandas Series\n",
    "        The training target variable.\n",
    "    X_val : pandas DataFrame\n",
    "        The validation features.\n",
    "    y_val : pandas Series\n",
    "        The validation target variable.\n",
    "    \n",
    "    Returns:\n",
    "    best_model : XGBClassifier\n",
    "        The XGBoost model with the best hyperparameters.\n",
    "    best_params : dict\n",
    "        The best hyperparameters found by Optuna.\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(lambda trial: objective(trial, X_train, y_train, X_val, y_val), n_trials=50)\n",
    "\n",
    "    best_params = study.best_trial.params\n",
    "    best_model = XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=len(y_train.unique()),\n",
    "        **best_params,\n",
    "        random_state=42\n",
    "    )\n",
    "    best_model.fit(X_train, y_train, verbose=False)\n",
    "    return best_model, best_params\n",
    "\n",
    "# Fine-tuning for other models using GridSearchCV\n",
    "def tune_model(X_train, y_train, model_type):\n",
    "    \"\"\"\n",
    "    Tunes hyperparameters of different models using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train : pandas DataFrame\n",
    "        The training features.\n",
    "    y_train : pandas Series\n",
    "        The training target variable.\n",
    "    model_type : str\n",
    "        The type of model to be tuned.\n",
    "    \n",
    "    Returns:\n",
    "    best_estimator : estimator\n",
    "        The model with the best hyperparameters.\n",
    "    \"\"\"\n",
    "    if model_type == \"xgb\":\n",
    "        return None\n",
    "    elif model_type == \"logreg\":\n",
    "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        param_grid = {\n",
    "            'C': [0.01, 0.1, 1, 10],\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs', 'liblinear']\n",
    "        }\n",
    "    elif model_type == \"rf\":\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    elif model_type == \"gb\":\n",
    "        model = GradientBoostingClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1],\n",
    "            'max_depth': [3, 6],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    elif model_type == \"knn\":\n",
    "        model = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'p': [1, 2]\n",
    "        }\n",
    "    elif model_type == \"et\":\n",
    "        model = ExtraTreesClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [10, 20],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    elif model_type == \"ada\":\n",
    "        model = AdaBoostClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.01, 0.1]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Model type not recognized\")\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f\"Best parameters for {model_type}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Evaluate model performance\n",
    "def evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the model on train, validation, and test datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    model : estimator\n",
    "        The trained model to evaluate.\n",
    "    X_train, X_val, X_test : pandas DataFrame\n",
    "        The feature sets for train, validation, and test data.\n",
    "    y_train, y_val, y_test : pandas Series\n",
    "        The target values for train, validation, and test data.\n",
    "    model_name : str\n",
    "        The name of the model being evaluated.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    log_loss_train = log_loss(y_train, model.predict_proba(X_train))\n",
    "    log_loss_val = log_loss(y_val, model.predict_proba(X_val))\n",
    "    log_loss_test = log_loss(y_test, model.predict_proba(X_test))\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, model.predict(X_train))\n",
    "    accuracy_val = accuracy_score(y_val, model.predict(X_val))\n",
    "    accuracy_test = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "    print(f\"{model_name} Train Log-Loss: {log_loss_train:.4f}\")\n",
    "    print(f\"{model_name} Validation Log-Loss: {log_loss_val:.4f}\")\n",
    "    print(f\"{model_name} Test Log-Loss: {log_loss_test:.4f}\")\n",
    "    print(f\"{model_name} Train Accuracy: {accuracy_train * 100:.2f}%\")\n",
    "    print(f\"{model_name} Validation Accuracy: {accuracy_val * 100:.2f}%\")\n",
    "    print(f\"{model_name} Test Accuracy: {accuracy_test * 100:.2f}%\")\n",
    "\n",
    "# Save predictions\n",
    "def make_predictions(model, df_encoded_test, encoder, model_name):\n",
    "    \"\"\"\n",
    "    Makes predictions on the test dataset and saves the results.\n",
    "    \n",
    "    Parameters:\n",
    "    model : estimator\n",
    "        The trained model to make predictions.\n",
    "    df_encoded_test : pandas DataFrame\n",
    "        The test data features.\n",
    "    encoder : transformer\n",
    "        The encoder used to transform the data.\n",
    "    model_name : str\n",
    "        The name of the model for saving the predictions.\n",
    "    \n",
    "    Returns:\n",
    "    output_filename : str\n",
    "        The path to the saved prediction file.\n",
    "    \"\"\"\n",
    "    ids = df_encoded_test['id']\n",
    "    df_encoded_test = encoder.transform(df_encoded_test.drop(columns=['id']))\n",
    "\n",
    "    y_test_pred = model.predict_proba(df_encoded_test)\n",
    "\n",
    "    output = pd.DataFrame(y_test_pred, columns=[f'class_{i}' for i in range(y_test_pred.shape[1])])\n",
    "    output['id'] = ids\n",
    "    output = output[['id'] + [f'class_{i}' for i in range(y_test_pred.shape[1])]]\n",
    "    \n",
    "    output_filename = data_dir / f\"predictions_{model_name}.csv\"\n",
    "    output.to_csv(output_filename, index=False)\n",
    "    print(f\"Predictions for {model_name} saved to {output_filename}\")\n",
    "\n",
    "    return output_filename\n",
    "\n",
    "# Encode data function\n",
    "def encode_data(train_df, test_df, encoder):\n",
    "    \"\"\"\n",
    "    Encodes the training and test datasets using the given encoder.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df : pandas DataFrame\n",
    "        The training dataset.\n",
    "    test_df : pandas DataFrame\n",
    "        The test dataset.\n",
    "    encoder : transformer\n",
    "        The encoder to transform the data.\n",
    "    \n",
    "    Returns:\n",
    "    df_encoded_train : pandas DataFrame\n",
    "        The encoded training dataset.\n",
    "    df_encoded_test : pandas DataFrame\n",
    "        The encoded test dataset.\n",
    "    \"\"\"\n",
    "    df_train = train_df.copy()\n",
    "    df_train.columns = df_train.columns.str.strip()\n",
    "    df_encoded_train = encoder.fit_transform(df_train)\n",
    "    \n",
    "    df_test = test_df.copy()\n",
    "    df_test.columns = df_test.columns.str.strip()\n",
    "    df_encoded_test = encoder.transform(df_test)\n",
    "    \n",
    "    return df_encoded_train, df_encoded_test\n",
    "\n",
    "# Create directories to save predictions\n",
    "def create_prediction_directories():\n",
    "    \"\"\"\n",
    "    Creates directories for saving the prediction files.\n",
    "    \n",
    "    Returns:\n",
    "    all_predictions_path : str\n",
    "        Path to the \"all-1\" directory.\n",
    "    best_predictions_path : str\n",
    "        Path to the \"best_predictions\" directory.\n",
    "    \"\"\"\n",
    "    base_path = data_dir / \"predictions\"\n",
    "    all_predictions_path = os.path.join(base_path, \"all-1\")\n",
    "    best_predictions_path = os.path.join(base_path, \"best_predictions\")\n",
    "\n",
    "    os.makedirs(all_predictions_path, exist_ok=True)\n",
    "    os.makedirs(best_predictions_path, exist_ok=True)\n",
    "\n",
    "    return all_predictions_path, best_predictions_path\n",
    "\n",
    "# Move prediction files to appropriate directories\n",
    "def move_predictions_to_directories(predictions, model_name, all_predictions_path, best_predictions_path):\n",
    "    \"\"\"\n",
    "    Moves prediction files to the appropriate directories based on the model.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions : dict\n",
    "        Dictionary containing model names and their respective prediction file paths.\n",
    "    model_name : str\n",
    "        The name of the model to move.\n",
    "    all_predictions_path : str\n",
    "        Path to the \"all-1\" directory.\n",
    "    best_predictions_path : str\n",
    "        Path to the \"best_predictions\" directory.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    best_model_filename = predictions[model_name]\n",
    "    for model_type, prediction_file in predictions.items():\n",
    "        if model_type == \"xgb\":\n",
    "            os.rename(prediction_file, os.path.join(best_predictions_path, os.path.basename(prediction_file)))\n",
    "        else:\n",
    "            os.rename(prediction_file, os.path.join(all_predictions_path, os.path.basename(prediction_file)))\n",
    "\n",
    "# Main pipeline\n",
    "def main(train_df, test_df, encoder):\n",
    "    \"\"\"\n",
    "    Main pipeline for training and evaluating models, and saving predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    train_df : pandas DataFrame\n",
    "        The training dataset.\n",
    "    test_df : pandas DataFrame\n",
    "        The test dataset.\n",
    "    encoder : transformer\n",
    "        The encoder used to transform the data.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    df_encoded_train, df_encoded_test = encode_data(train_df, test_df, encoder)\n",
    "    \n",
    "    X, y = prepare_data(df_encoded_train)\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "\n",
    "    all_predictions_path, best_predictions_path = create_prediction_directories()\n",
    "\n",
    "    predictions = {}\n",
    "\n",
    "    model_types = [\"logreg\", \"rf\", \"gb\", \"knn\", \"et\", \"ada\"] \n",
    "    for model_type in model_types:\n",
    "        print(f\"Training {model_type} model...\")\n",
    "        model = tune_model(X_train, y_train, model_type) \n",
    "        evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test, model_type)\n",
    "        pred_filename = make_predictions(model, df_encoded_test, encoder, model_type)\n",
    "        predictions[model_type] = pred_filename\n",
    "\n",
    "    print(\"Training XGBoost with Optuna...\")\n",
    "    best_xgb_model, best_params = tune_xgb_with_optuna(X_train, y_train, X_val, y_val)\n",
    "    evaluate_model(best_xgb_model, X_train, X_val, X_test, y_train, y_val, y_test, \"xgb\")\n",
    "    xgb_pred_filename = make_predictions(best_xgb_model, df_encoded_test, encoder, \"xgb\")\n",
    "    predictions[\"xgb\"] = xgb_pred_filename\n",
    "    \n",
    "    move_predictions_to_directories(predictions, \"xgb\", all_predictions_path, best_predictions_path)\n",
    "\n",
    "# Call the main function\n",
    "main(train_df, test_df, encoder)\n",
    "\n",
    "\n",
    "##Disabling error messages and all useless information\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "# Desactivate logs of Optuna\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why XGBoost is the Best\n",
    "\n",
    "As expected, **XGBoost** outperformed the other models with an accuracy of **70%**, making it the best-performing model for this task. Several factors contribute to XGBoost's superior performance:\n",
    "\n",
    "1.  **Boosting Technique**: XGBoost uses the **gradient boosting** framework, which builds an ensemble of weak models (decision trees) sequentially. Each subsequent model corrects the errors made by previous ones, leading to better generalization and higher accuracy.\n",
    "    \n",
    "2.  **Regularization**: XGBoost includes built-in regularization (L1 and L2), which helps to prevent overfitting, a common issue with tree-based models. This leads to more robust and accurate predictions, especially on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Development\n",
    "\n",
    "While XGBoost has delivered solid results, there are several opportunities to further enhance its performance:\n",
    "\n",
    "1.  **Increase the Number of Optimization Steps**: Currently, the hyperparameter tuning process using Optuna has a limited number of trials (50). Increasing this number would provide a deeper exploration of the hyperparameter space and might lead to a better combination of parameters, further improving the model's performance.\n",
    "    \n",
    "2.  **Use a More Powerful Optimizer**: Although Optuna is a powerful optimization tool, exploring other advanced optimizers like\n",
    "    \n",
    "    ADAM could potentially improve the model by using more sophisticated strategies to search the hyperparameter space. These methods could help avoid local minima and better explore the global optimum.\n",
    "    \n",
    "3.  **Feature Engineering**: Exploring additional feature engineering techniques (e.g., interaction features or polynomial features) could help capture more complex patterns in the data, improving model performance.\n",
    "    \n",
    "4.  **Model Interpretability**: While XGBoost is already interpretable, implementing further interpretability techniques, such as SHAP (SHapley Additive exPlanations), could provide more insights into how the model is making decisions and ensure the model’s decisions are more transparent and explainable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
